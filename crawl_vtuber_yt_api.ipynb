{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! pip install pandas\n",
    "! pip install googleapiclient"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from googleapiclient.discovery import build\n",
    "from googleapiclient.errors import HttpError\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## FUNCTIONS ##############\n",
    "# YouTube channels IDs of list of users\n",
    "def get_channels(vtubers):\n",
    "    channels = []\n",
    "\n",
    "    for vtuber in vtubers['jp_name']:\n",
    "        res = youtube.search().list(q = vtuber, \n",
    "                                    part = \"snippet\", \n",
    "                                    type = \"channel\", \n",
    "                                    maxResults = 1).execute()\n",
    "        channel_id = res['items'][0]['id']['channelId']\n",
    "        channels.append(channel_id)\n",
    "        \n",
    "    else:\n",
    "        print('All channel IDs retrieved!')\n",
    "        return channels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# retrieve meta-information and list of all videos for each YT channel in a list of channels\n",
    "def get_channel_info(channels):\n",
    "    debut = []\n",
    "    subs = []\n",
    "    views = []\n",
    "    vid_cnt = []\n",
    "    videos = {}\n",
    "    \n",
    "    for channel in channels:\n",
    "        # create a key for each channel which takes a list of video IDs as its value\n",
    "        videos[channel] = []\n",
    "\n",
    "        # extract playlist ID including all uploaded videos\n",
    "        res = youtube.channels().list(id = channel, \n",
    "                                      part = 'snippet, statistics, contentDetails').execute()\n",
    "        all_uploads = res['items'][0]['contentDetails']['relatedPlaylists']['uploads']\n",
    "        \n",
    "        # record meta-information\n",
    "        debut.append(res['items'][0]['snippet']['publishedAt'])\n",
    "        subs.append(int(res['items'][0]['statistics']['subscriberCount']))\n",
    "        views.append(int(res['items'][0]['statistics']['viewCount']))\n",
    "        vid_cnt.append(int(res['items'][0]['statistics']['videoCount']))\n",
    "        \n",
    "        # record all video IDs\n",
    "        next_page_token = None\n",
    "        while True:\n",
    "            res = youtube.playlistItems().list(playlistId = all_uploads, \n",
    "                                               part = 'contentDetails', \n",
    "                                               pageToken = next_page_token, \n",
    "                                               maxResults = 50).execute()\n",
    "            for video in res['items']:\n",
    "                videos[channel].append(video['contentDetails']['videoId'])\n",
    "            next_page_token = res.get('nextPageToken')\n",
    "            \n",
    "            if next_page_token is None:\n",
    "                break\n",
    "    \n",
    "    else:\n",
    "        print('All channel information has been recorded!')\n",
    "        return debut, subs, views, vid_cnt, videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate dataframe containing video information from a list of video IDs\n",
    "def get_video_info(videos):\n",
    "    data = {'vid_id': [], 'title': [], 'description': [], 'category': [], 'date': [], 'duration': [], '2d3d': [], \n",
    "            'definition': [], 'views': [], 'likes': [], 'dislikes': [], 'favorites': [], 'num_comments': []}\n",
    "\n",
    "    for video in videos:\n",
    "        res = youtube.videos().list(id = video, \n",
    "                                    part = 'snippet, contentDetails, statistics').execute()\n",
    "\n",
    "        data['vid_id'].append(video)\n",
    "        data['title'].append(res['items'][0]['snippet']['title'])\n",
    "        data['description'].append(res['items'][0]['snippet']['description'])\n",
    "        data['category'].append(int(res['items'][0]['snippet']['categoryId']))    # 필요할까?\n",
    "        data['date'].append(res['items'][0]['snippet']['publishedAt'])    # datetime\n",
    "        data['duration'].append(res['items'][0]['contentDetails']['duration'])    # string\n",
    "        data['2d3d'].append(res['items'][0]['contentDetails']['dimension'])\n",
    "        data['definition'].append(res['items'][0]['contentDetails']['definition'])\n",
    "        \n",
    "        views = res['items'][0]['statistics'].get('viewCount')\n",
    "        if views is None:\n",
    "            views = 0\n",
    "        data['views'].append(int(views))\n",
    "        \n",
    "        likes = res['items'][0]['statistics'].get('likeCount')\n",
    "        if likes is None:\n",
    "            likes = 0\n",
    "        data['likes'].append(int(likes))\n",
    "        \n",
    "        dislikes = res['items'][0]['statistics'].get('dislikeCount')\n",
    "        if dislikes is None:\n",
    "            dislikes = 0\n",
    "        data['dislikes'].append(int(dislikes))\n",
    "        \n",
    "        favorites = res['items'][0]['statistics'].get('favoriteCount')\n",
    "        if favorites is None:\n",
    "            favorites = 0\n",
    "        data['favorites'].append(int(favorites))\n",
    "\n",
    "        num_comments = res['items'][0]['statistics'].get('commentCount')\n",
    "        if num_comments is None:\n",
    "            num_comments = 0\n",
    "        data['num_comments'].append(int(num_comments))\n",
    "\n",
    "    else:\n",
    "        video_info = pd.DataFrame(data)\n",
    "        print('All video information extracted!')\n",
    "        return video_info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def divide_and_conquer(batch, final_vtubers, channel2videos):\n",
    "    # add channel ID to dataframe\n",
    "    channels = get_channels(batch)\n",
    "    batch['channel_id'] = channels\n",
    "\n",
    "    # add meta-information to dataframe\n",
    "    debut, subs, views, vid_cnt, videos = get_channel_info(channels)\n",
    "    batch['debut'] = debut\n",
    "    batch['total_subs'] = subs\n",
    "    batch['total_views'] = views\n",
    "    batch['vid_cnt'] = vid_cnt\n",
    "    \n",
    "    # add records to final version\n",
    "    final_vtubers = final_vtubers.append(batch)\n",
    "    channel2videos.update(videos)\n",
    "    \n",
    "    return final_vtubers, channel2videos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "############## MAIN ##############\n",
    "vtuber_rank_file = '[20200526] vtuber_all_ranking.csv'\n",
    "top100 = pd.read_csv(vtuber_rank_file, encoding = 'utf-8', sep = ',', index_col = 'ranking')[:100]\n",
    "# batches = [top100[:5], top100[5:10], top100[10:15], top100[15:20], top100[20:25], top100[25:30], top100[30:35], top100[35:40], \n",
    "#            top100[40:45], top100[45:50], top100[50:55], top100[55:60], top100[60:65], top100[65:70], top100[70:75], top100[75:80], \n",
    "#            top100[80:85], top100[85:90], top100[90:95], top100[95:100]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# build Youtube Data v3 API\n",
    "DEVELOPER_KEY = 'AIzaSyCWsLADiiwZg0zGRVKvhuzqDjHri56ZVk8'\n",
    "\n",
    "API_SERVICE_NAME = 'youtube'\n",
    "API_VERSION = 'v3'\n",
    "\n",
    "youtube = build(API_SERVICE_NAME, API_VERSION, developerKey = DEVELOPER_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# initialize base for data recording\n",
    "final_vtubers = pd.DataFrame()\n",
    "channel2videos = dict()\n",
    "final_videos = pd.DataFrame()\n",
    "lower = 90"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #### 이 아래 구문들은 for문 대신 수동으로 각  batch를 돌리길 권장합니다!!! (quota issues)\n",
    "# for batch in batches:\n",
    "#     final_vtubers, channel2videos = divide_and_conquer(batch, final_vtubers, channel2videos)\n",
    "\n",
    "# for channel, videos in channel2videos:\n",
    "#     video_info = get_video_info(videos)\n",
    "#     video_info['channel_id'] = channel\n",
    "#     final_videos = final_videos.append(video_info)\n",
    "\n",
    "# # record collected data to an external file    \n",
    "# final_vtubers.to_csv('final_vtubers_50_60.csv', encoding = 'utf-8')\n",
    "# final_videos.to_csv('final_videos_50_60.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All channel IDs retrieved!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-b36647e6dc77>:4: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['channel_id'] = channels\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All channel information has been recorded!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-45-b36647e6dc77>:8: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['debut'] = debut\n",
      "<ipython-input-45-b36647e6dc77>:9: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['total_subs'] = subs\n",
      "<ipython-input-45-b36647e6dc77>:10: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['total_views'] = views\n",
      "<ipython-input-45-b36647e6dc77>:11: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  batch['vid_cnt'] = vid_cnt\n"
     ]
    }
   ],
   "source": [
    "final_vtubers, channel2videos = divide_and_conquer(top100[lower:lower + 10], final_vtubers, channel2videos)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-55-c8bc140ef8b3>:3: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  final_vtubers['total_subs'][88] = 186000\n"
     ]
    }
   ],
   "source": [
    "# conduct the following operation for アズマ リム(アズリム) \n",
    "# data error in api retrieval (system problem)\n",
    "\n",
    "# final_vtubers['total_subs'][88] = 186000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "fname = f'final_vtubers_{lower}_{lower + 10}.csv'\n",
    "final_vtubers.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10\n"
     ]
    }
   ],
   "source": [
    "# extract channel IDs and check length of list\n",
    "channels = list()\n",
    "for channel in channel2videos:\n",
    "    channels.append(channel)\n",
    "print(len(channels))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "# get video information from a single channel\n",
    "video_info = get_video_info(channel2videos[channels[0]])\n",
    "video_info['channel_id'] = channels[0]\n",
    "\n",
    "# save video information to an external file\n",
    "fname = f'final_videos_{lower}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 188,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[1]])\n",
    "video_info['channel_id'] = channels[1]\n",
    "\n",
    "fname = f'final_videos_{lower + 1}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 189,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[2]])\n",
    "video_info['channel_id'] = channels[2]\n",
    "\n",
    "fname = f'final_videos_{lower + 2}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[3]])\n",
    "video_info['channel_id'] = channels[3]\n",
    "\n",
    "fname = f'final_videos_{lower + 3}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 202,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[4]])\n",
    "video_info['channel_id'] = channels[4]\n",
    "\n",
    "fname = f'final_videos_{lower + 4}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 203,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[5]])\n",
    "video_info['channel_id'] = channels[5]\n",
    "\n",
    "fname = f'final_videos_{lower + 5}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 204,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[6]])\n",
    "video_info['channel_id'] = channels[6]\n",
    "\n",
    "fname = f'final_videos_{lower + 6}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 205,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[7]])\n",
    "video_info['channel_id'] = channels[7]\n",
    "\n",
    "fname = f'final_videos_{lower + 7}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[8]])\n",
    "video_info['channel_id'] = channels[8]\n",
    "\n",
    "fname = f'final_videos_{lower + 8}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 209,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All video information extracted!\n"
     ]
    }
   ],
   "source": [
    "video_info = get_video_info(channel2videos[channels[9]])\n",
    "video_info['channel_id'] = channels[9]\n",
    "\n",
    "fname = f'final_videos_{lower + 9}.csv'\n",
    "video_info.to_csv(fname, encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Data Accumulation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulate vtuber data and record on external file\n",
    "columns = ['ranking', 'jp_name', 'eng_name', 'agency', 'total_subs', 'total_views', 'channel_id', 'debut', 'vid_cnt']\n",
    "final_vtubers = pd.DataFrame(columns = columns)\n",
    "\n",
    "# files for rank1~50 (processed by 정걸)\n",
    "fnames = ['final_vtubers_0_4.csv', \n",
    "          'final_vtubers_1_2.csv', \n",
    "          'final_vtubers_2_3.csv', \n",
    "          'final_vtubers_3_4.csv', \n",
    "          'final_vtubers_4_6.csv', \n",
    "          'final_vtubers_5_8.csv', \n",
    "          'final_vtubers_7_10.csv', \n",
    "          'final_vtubers_8_10.csv', \n",
    "          'final_vtubers_9_10.csv', \n",
    "          'final_vtubers_10_12.csv', \n",
    "          'final_vtubers_11_12.csv', \n",
    "          'final_vtubers_12_22.csv', \n",
    "          'final_vtubers_14_17.csv', \n",
    "          'final_vtubers_15_18.csv', \n",
    "          'final_vtubers_18_20.csv', \n",
    "          'final_vtubers_20_30.csv', \n",
    "          'final_vtubers_24_26.csv', \n",
    "          'final_vtubers_26_28.csv', \n",
    "          'final_vtubers_27_29.csv', \n",
    "          'final_vtubers_28_30.csv', \n",
    "          'final_vtubers_30_40.csv', \n",
    "          'final_vtubers_31_33.csv', \n",
    "          'final_vtubers_33_35.csv', \n",
    "          'final_vtubers_35_37.csv', \n",
    "          'final_vtubers_37_39.csv', \n",
    "          'final_vtubers_39_41.csv', \n",
    "          'final_vtubers_40_50.csv', \n",
    "          'final_vtubers_42_44.csv', \n",
    "          'final_vtubers_44_46.csv', \n",
    "          'final_vtubers_46_48.csv', \n",
    "          'final_vtubers_48_50.csv', \n",
    "          'final_vtubers_49_51.csv']\n",
    "\n",
    "for fname in fnames:\n",
    "    vtubers_temp = pd.read_csv(fname, encoding = 'utf-8', sep = ',').to_numpy()\n",
    "    for row in vtubers_temp:\n",
    "        if row[0] not in list(final_vtubers.ranking):\n",
    "            final_vtubers = final_vtubers.append(pd.DataFrame([row], columns = columns))\n",
    "\n",
    "# files for rank 51~100 (processed by 박형서)\n",
    "lower = 50\n",
    "\n",
    "while lower <= 90:\n",
    "    vtubers_fname = f'final_vtubers_{lower}_{lower + 10}.csv'\n",
    "    vtubers_temp = pd.read_csv(vtubers_fname, encoding = 'utf-8', sep = ',')\n",
    "    final_vtubers = final_vtubers.append(vtubers_temp)\n",
    "    lower += 10\n",
    "\n",
    "final_vtubers = final_vtubers.set_index('ranking')\n",
    "final_vtubers.to_csv('final_vtubers.csv', encoding = 'utf-8')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "# accumulate video data and record on external file\n",
    "final_videos = pd.DataFrame()\n",
    "columns = ['vid_id', 'title', 'description', 'category', 'date', 'duration', '2d3d', 'definition', \n",
    "           'views', 'likes', 'dislikes', 'favorites', 'num_comments', 'channel_id']\n",
    "\n",
    "i = 0    \n",
    "\n",
    "while i < 100:\n",
    "    videos_fname = f'final_videos_{i}.csv'\n",
    "    videos_temp = pd.read_csv(videos_fname, encoding = 'utf-8', sep = ',')\n",
    "    final_videos = final_videos.append(videos_temp)\n",
    "    i += 1\n",
    "\n",
    "final_videos = pd.DataFrame(final_videos, columns = columns).set_index('vid_id')\n",
    "final_videos.to_csv('final_videos.csv', encoding = 'utf-8')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
